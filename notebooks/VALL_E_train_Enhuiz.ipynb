{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7ATBTlJpH38V",
        "zf6qnSQwGOCw",
        "MkTFZSkKF9M2",
        "p6j2RLvfJWPN",
        "wvrnSKFfHhUR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "7ATBTlJpH38V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install AudioConverter"
      ],
      "metadata": {
        "id": "SYhOMQpiH5y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed==0.8.3"
      ],
      "metadata": {
        "id": "pRTUXzQOMkF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "zf6qnSQwGOCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "owuueC-ZGQpI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile"
      ],
      "metadata": {
        "id": "18WnnvKyHYiU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "6WIYdki9MPF5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the repo and install dependencies"
      ],
      "metadata": {
        "id": "MkTFZSkKF9M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/enhuiz/vall-e.git"
      ],
      "metadata": {
        "id": "wRE4j7CKE_Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd vall-e"
      ],
      "metadata": {
        "id": "vuouyAvhFEkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "TMOeLp1C_Oxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "L7_SpCUNFpNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and process the librilight data"
      ],
      "metadata": {
        "id": "bymh4U1PGEfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/librilight/data/librispeech_finetuning.tgz"
      ],
      "metadata": {
        "id": "j25JNMugFqT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # open file\n",
        "file = tarfile.open('librispeech_finetuning.tgz')\n",
        "\n",
        "# extracting file\n",
        "file.extractall('./')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "9nHR2yJNF4y6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vall-e/data/libri"
      ],
      "metadata": {
        "id": "-6spcTg3GaCm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in os.listdir('librispeech_finetuning/9h/clean'):\n",
        "    for j in os.listdir('librispeech_finetuning/9h/'+'clean'):\n",
        "        for z in os.listdir('librispeech_finetuning/9h/clean/'+j):\n",
        "            for i in os.listdir('librispeech_finetuning/9h/clean/'+j+'/'+z):\n",
        "                os.rename('librispeech_finetuning/9h/clean/'+j+'/'+z+'/'+i, f'vall-e/data/libri/{i}')\n",
        "        \n",
        "    break\n",
        "lst = []\n",
        "for i in os.listdir('vall-e/data/libri'):\n",
        "    try:\n",
        "        if 'trans' in i:\n",
        "            with open(f'vall-e/data/libri/{i}') as text_file:\n",
        "                for row in text_file:\n",
        "                    z = row.split('-')\n",
        "                    name = z[0]+'-'+z[1]+ '-' + z[2].split(' ')[0]\n",
        "                    text = \" \".join(z[2].split(' ')[1:])\n",
        "                    # print(name, text)\n",
        "                    lst.append([name, text])\n",
        "\n",
        "                        \n",
        "    except:\n",
        "        None      \n",
        "\n",
        "for i in lst:\n",
        "    try:\n",
        "        with open('vall-e/data/libri/'+i[0]+'.txt', 'x') as file:\n",
        "            # print(i[1])\n",
        "            file.write(i[1])\n",
        "    except:\n",
        "        with open('vall-e/data/libri/'+i[0]+'.txt', 'w+') as file:\n",
        "            # print(i[1])\n",
        "            file.write(i[1])\n",
        "\n",
        "for i in sorted(os.listdir('vall-e/data/libri')):\n",
        "    if i.split('.')[1] == 'txt':\n",
        "        # print(i)\n",
        "        print('.'.join([i.split('.')[0],'normalized',i.split('.')[1]]))\n",
        "        os.rename('vall-e/data/libri/'+i, 'vall-e/data/libri/'+'.'.join([i.split('.')[0],'normalized',i.split('.')[1]]))"
      ],
      "metadata": {
        "id": "UioOe_h3G1rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!audioconvert convert vall-e/data/libri/ vall-e/data/libri --output-format .wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQN_zz_9IDwY",
        "outputId": "6ced65eb-2564-4ff6-f54b-8efb344ecfa5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m[ INFO    ] Starting conversion of vall-e/data/libri/.\u001b[0m\n",
            "\u001b[32m[ SUCCESS ] See vall-e/data/libri for converted audio.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize the recordings and generate phonemes"
      ],
      "metadata": {
        "id": "p6j2RLvfJWPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd vall-e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSo-ZhRzJlm7",
        "outputId": "89d3a010-1f8f-42e6-b7c1-c9421ccb7625"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vall-e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.emb.qnt data/libri"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKlrKbf6JnYW",
        "outputId": "7c621126-7afe-469c-ac23-0824db402229"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/1222 [00:00<?, ?it/s]Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
            "\n",
            "  0% 0.00/88.9M [00:00<?, ?B/s]\u001b[A\n",
            "  9% 7.92M/88.9M [00:00<00:01, 83.1MB/s]\u001b[A\n",
            " 28% 25.1M/88.9M [00:00<00:00, 140MB/s] \u001b[A\n",
            " 47% 41.8M/88.9M [00:00<00:00, 156MB/s]\u001b[A\n",
            " 66% 58.7M/88.9M [00:00<00:00, 164MB/s]\u001b[A\n",
            "100% 88.9M/88.9M [00:00<00:00, 158MB/s]\n",
            "100% 1222/1222 [01:37<00:00, 12.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.emb.g2p data/libri"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6noBGW8wJ7-W",
        "outputId": "8f55bf0b-da6c-4376-80e1-6c7fdf45718f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "100% 1222/1222 [00:10<00:00, 119.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the AR model\n",
        "\n",
        "- Modify the config files as needed"
      ],
      "metadata": {
        "id": "EZTIZC8qLAcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p zoo"
      ],
      "metadata": {
        "id": "zVHgEX6AL0-H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.train yaml=config/test/ar.yml"
      ],
      "metadata": {
        "id": "t-fIB8UTLMEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.export zoo/ar.pt yaml=config/test/ar.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ems-lyb4LxmS",
        "outputId": "fde23393-2c4c-4515-bd1a-fea4b998465f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-05-22 04:07:56,372] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2023-05-22 04:07:58,258] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.10143852233886719 seconds\n",
            "[2023-05-22 04:07:59,218] [INFO] [logging.py:93:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2023-05-22 04:07:59,221] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2023-05-22 04:07:59,221] [INFO] [logging.py:93:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
            "[2023-05-22 04:07:59,228] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
            "[2023-05-22 04:07:59,229] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR\n",
            "[2023-05-22 04:07:59,229] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fc02f0be5c0>\n",
            "[2023-05-22 04:07:59,229] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]\n",
            "[2023-05-22 04:07:59,229] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   amp_enabled .................. False\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   amp_params ................... False\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc02f0be020>\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   communication_data_type ...... None\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2023-05-22 04:07:59,230] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   disable_allgather ............ False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   dump_state ................... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... None\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   fp16_enabled ................. True\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   global_rank .................. 0\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   gradient_clipping ............ 100.0\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\n",
            "[2023-05-22 04:07:59,231] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 65536\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   loss_scale ................... 0\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   memory_breakdown ............. False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   optimizer_name ............... adam\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   optimizer_params ............. None\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   pld_enabled .................. False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   pld_params ................... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   prescale_gradients ........... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   scheduler_name ............... WarmupDecayLR\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   scheduler_params ............. {'warmup_min_lr': 1e-06, 'warmup_max_lr': 0.0002, 'warmup_num_steps': 1000, 'total_num_steps': 1000, 'warmup_type': 'linear'}\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   sparse_attention ............. None\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   steps_per_print .............. 10\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   train_batch_size ............. 32\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  32\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   world_size ................... 1\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\n",
            "[2023-05-22 04:07:59,232] [INFO] [config.py:1022:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
            "[2023-05-22 04:07:59,233] [INFO] [config.py:1022:print]   zero_enabled ................. False\n",
            "[2023-05-22 04:07:59,233] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2023-05-22 04:07:59,233] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 0\n",
            "[2023-05-22 04:07:59,233] [INFO] [config.py:1007:print_user_config]   json = {\n",
            "    \"train_micro_batch_size_per_gpu\": 32, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"lr\": 1e-06\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupDecayLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 1e-06, \n",
            "            \"warmup_max_lr\": 0.0002, \n",
            "            \"warmup_num_steps\": 1000, \n",
            "            \"total_num_steps\": 1000, \n",
            "            \"warmup_type\": \"linear\"\n",
            "        }\n",
            "    }, \n",
            "    \"gradient_clipping\": 100.0, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.09636402130126953 seconds\n",
            "[2023-05-22 04:07:59,330] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loading checkpoint from ckpts/test/ar/model/default/mp_rank_00_model_states.pt...\n",
            "[2023-05-22 04:07:59,435] [INFO] [torch_checkpoint_engine.py:25:load] [Torch] Loaded checkpoint from ckpts/test/ar/model/default/mp_rank_00_model_states.pt.\n",
            "[2023-05-22 04:07:59,442] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loading checkpoint from ckpts/test/ar/model/default/mp_rank_00_model_states.pt...\n",
            "[2023-05-22 04:07:59,536] [INFO] [torch_checkpoint_engine.py:25:load] [Torch] Loaded checkpoint from ckpts/test/ar/model/default/mp_rank_00_model_states.pt.\n",
            "1222it [00:00, 73382.00it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "zoo/ar.pt saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the NAR model"
      ],
      "metadata": {
        "id": "53Cy8dYjLM7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.train yaml=config/test/nar.yml"
      ],
      "metadata": {
        "id": "mPA54-oILdtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e.export zoo/nar.pt yaml=config/test/nar.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEI-eQpJL5pk",
        "outputId": "62d9b3ce-fda4-456a-90d5-110d1d503bac"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-05-22 03:38:50,504] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2023-05-22 03:38:52,308] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.09243226051330566 seconds\n",
            "[2023-05-22 03:38:53,237] [INFO] [logging.py:93:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2023-05-22 03:38:53,239] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2023-05-22 03:38:53,239] [INFO] [logging.py:93:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
            "[2023-05-22 03:38:53,246] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
            "[2023-05-22 03:38:53,246] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR\n",
            "[2023-05-22 03:38:53,246] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fc44913e5f0>\n",
            "[2023-05-22 03:38:53,246] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]\n",
            "[2023-05-22 03:38:53,246] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   amp_enabled .................. False\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   amp_params ................... False\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\n",
            "[2023-05-22 03:38:53,247] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc44913e170>\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   communication_data_type ...... None\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   disable_allgather ............ False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   dump_state ................... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... None\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   fp16_enabled ................. True\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   global_rank .................. 0\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\n",
            "[2023-05-22 03:38:53,248] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   gradient_clipping ............ 100.0\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 65536\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   loss_scale ................... 0\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   memory_breakdown ............. False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   optimizer_name ............... adam\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   optimizer_params ............. None\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   pld_enabled .................. False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   pld_params ................... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   prescale_gradients ........... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   scheduler_name ............... WarmupDecayLR\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   scheduler_params ............. {'warmup_min_lr': 1e-06, 'warmup_max_lr': 0.0002, 'warmup_num_steps': 1000, 'total_num_steps': 1000, 'warmup_type': 'linear'}\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   sparse_attention ............. None\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   steps_per_print .............. 10\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   train_batch_size ............. 32\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  32\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\n",
            "[2023-05-22 03:38:53,249] [INFO] [config.py:1022:print]   world_size ................... 1\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1022:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1022:print]   zero_enabled ................. False\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 0\n",
            "[2023-05-22 03:38:53,250] [INFO] [config.py:1007:print_user_config]   json = {\n",
            "    \"train_micro_batch_size_per_gpu\": 32, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"lr\": 1e-06\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupDecayLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 1e-06, \n",
            "            \"warmup_max_lr\": 0.0002, \n",
            "            \"warmup_num_steps\": 1000, \n",
            "            \"total_num_steps\": 1000, \n",
            "            \"warmup_type\": \"linear\"\n",
            "        }\n",
            "    }, \n",
            "    \"gradient_clipping\": 100.0, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.09576940536499023 seconds\n",
            "[2023-05-22 03:38:53,346] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loading checkpoint from ckpts/test/nar/model/default/mp_rank_00_model_states.pt...\n",
            "[2023-05-22 03:38:53,459] [INFO] [torch_checkpoint_engine.py:25:load] [Torch] Loaded checkpoint from ckpts/test/nar/model/default/mp_rank_00_model_states.pt.\n",
            "[2023-05-22 03:38:53,468] [INFO] [torch_checkpoint_engine.py:23:load] [Torch] Loading checkpoint from ckpts/test/nar/model/default/mp_rank_00_model_states.pt...\n",
            "[2023-05-22 03:38:53,572] [INFO] [torch_checkpoint_engine.py:25:load] [Torch] Loaded checkpoint from ckpts/test/nar/model/default/mp_rank_00_model_states.pt.\n",
            "1222it [00:00, 69892.67it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "zoo/nar.pt saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a new sample"
      ],
      "metadata": {
        "id": "h0hSSZL4L85C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vall_e 'hello world' data/test/test.wav toy1.wav"
      ],
      "metadata": {
        "id": "kWR87s3PMM6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio('toy.wav')"
      ],
      "metadata": {
        "id": "CacxRBBhMReN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility"
      ],
      "metadata": {
        "id": "wvrnSKFfHhUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ecKVnuBuHd8R",
        "outputId": "67cbba82-c3bf-46b3-bc31-4c60afd1329f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/vall-e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data/libri.zip data/libri/"
      ],
      "metadata": {
        "id": "sNXRKF7OI2jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in os.listdir('data/libri'):\n",
        "  print('size: '+ str(os.path.getsize('data/libri/'+f)))"
      ],
      "metadata": {
        "id": "j4gPF4-mXAhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OL2AO9xaYulH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}